# ğŸ“˜ Moteur de Recommandation de Livres (IA + MongoDB + Streamlit)

## ğŸ“š PrÃ©sentation du projet

Ce projet consiste Ã  dÃ©velopper une application web intelligente permettant de recommander des livres grÃ¢ce Ã  la recherche sÃ©mantique. L'utilisateur saisit une phrase dÃ©crivant le type de livre recherchÃ©, et l'application renvoie automatiquement les Å“uvres les plus pertinentes en se basant sur le sens de la phrase, et non sur des mots-clÃ©s.

Le systÃ¨me utilise :
* **MongoDB Atlas** pour stocker une base de livres
* **Sentence Transformers** pour encoder les descriptions en vecteurs
* La **similaritÃ© cosinus** pour comparer la requÃªte aux livres
* **Streamlit** pour l'interface utilisateur

Ce projet montre comment crÃ©er un moteur de recommandation moderne basÃ© sur l'IA et les embeddings.

---

## ğŸ§  Logique gÃ©nÃ©rale du moteur de recommandation

### 1. Chargement des donnÃ©es
L'application rÃ©cupÃ¨re les livres dans MongoDB (Title, Author, Category, Description, Year, Rating).

### 2. Vectorisation
Chaque description est transformÃ©e en vecteur numÃ©rique (embedding) Ã  l'aide du modÃ¨le `paraphrase-multilingual-mpnet-base-v2`.

### 3. RequÃªte utilisateur
Le texte saisi est Ã©galement converti en embedding.

### 4. Calcul de similaritÃ© cosinus
On compare l'embedding de la requÃªte avec tous les embeddings des livres pour mesurer leur proximitÃ© sÃ©mantique.

#### ğŸ”¢ Formule utilisÃ©e (similaritÃ© cosinus)

$$\cos(\theta) = \frac{\vec{q} \cdot \vec{d}}{\|\vec{q}\| \, \|\vec{d}\|}$$

Avec :

* **Produit scalaire** :
  
  $$\vec{q} \cdot \vec{d} = \sum_{i=1}^{n} q_i d_i$$

* **Normes** :
  
  $$\|\vec{q}\| = \sqrt{\sum_{i=1}^{n} q_i^2}$$
  
  $$\|\vec{d}\| = \sqrt{\sum_{i=1}^{n} d_i^2}$$

Le rÃ©sultat est compris entre :
* **+1** â†’ trÃ¨s similaire
* **0** â†’ pas de lien
* **-1** â†’ opposÃ© (rare pour ce type d'embeddings)

### 5. Tri des rÃ©sultats
Les livres sont classÃ©s du plus pertinent au moins pertinent et affichÃ©s dans l'interface.

---

# ğŸ“Œ ModÃ¨le utilisÃ© : paraphrase-multilingual-mpnet-base-v2

## ğŸ”¹ Description gÃ©nÃ©rale

`paraphrase-multilingual-mpnet-base-v2` est un modÃ¨le **Sentence Transformers** qui transforme des phrases en vecteurs numÃ©riques reprÃ©sentant leur sens, permettant une comparaison sÃ©mantique entre textes.

---

## ğŸŒ ModÃ¨le multilingue

Le modÃ¨le comprend plus de **50 langues**, dont le franÃ§ais. Il gÃ¨re efficacement des descriptions de livres variÃ©es, quel que soit le style, la longueur ou la langue utilisÃ©e.

---

## ğŸ§  BasÃ© sur MPNet

Construit sur l'architecture **MPNet**, une version amÃ©liorÃ©e de BERT, le modÃ¨le offre :
- une meilleure comprÃ©hension du contexte
- une cohÃ©rence sÃ©mantique plus forte
- des reprÃ©sentations vectorielles plus riches

---

## ğŸ” OptimisÃ© pour la similaritÃ© sÃ©mantique

EntraÃ®nÃ© sur des paires de phrases paraphrasÃ©es, il peut :
- dÃ©tecter des textes ayant le mÃªme sens
- mesurer leur similaritÃ©
- produire des embeddings directement comparables via la **similaritÃ© cosinus**

---

## ğŸ—‚ï¸ Structure du projet
```
.
â”œâ”€â”€ livre_app.py        # Application Streamlit
â”œâ”€â”€ README.md           # Documentation du projet
â””â”€â”€ screenshots/        # Captures de l'application
```

---

## âš™ï¸ Ã‰tapes du projet

### âœ”ï¸ 1. CrÃ©ation de la base MongoDB
* Cluster Atlas
* Base `livre_database`
* Collection `livres`
* Import des documents JSON

![Base de donnÃ©es de livres dans mongo Atlas](screenshots/livres_atlas_data.png)


### âœ”ï¸ 2. DÃ©veloppement de l'IA
* Chargement du modÃ¨le SentenceTransformer
* Vectorisation des descriptions
* Calcul des similaritÃ©s cosinus

### âœ”ï¸ 3. CrÃ©ation de l'interface Streamlit
* Barre de recherche
* RÃ©sultats affichÃ©s proprement
* Pertinence, auteur, catÃ©gorie, rÃ©sumÃ©â€¦

![DÃ©mo_application](screenshots/capture_application.png)

### âœ”ï¸ 4. Test et dÃ©ploiement local

---

## â–¶ï¸ Comment exÃ©cuter l'application

### 1ï¸âƒ£ Activer l'environnement virtuel

**Windows PowerShell :**
```powershell
.\env\Scripts\Activate.ps1
```

### 2ï¸âƒ£ Lancer l'application
```bash
streamlit run livre_app.py
```

---

## ğŸ“¦ DÃ©pendances principales

* `streamlit`
* `pymongo`
* `sentence-transformers`
* `scikit-learn`
* `numpy`

---

# MÃ©thode 2 : Recherche Vectorielle via MongoDB Atlas

**Alternative avancÃ©e pour la recherche sÃ©mantique**

---

## ğŸ“– Description

Cette seconde approche remplace la recherche sÃ©mantique locale (calculÃ©e en Python avec la similaritÃ© cosinus) par une **recherche vectorielle** rÃ©alisÃ©e directement dans **MongoDB Atlas**, grÃ¢ce Ã  un index vectoriel optimisÃ©.

---

## â­ Points clÃ©s de cette mÃ©thode

### ğŸ—„ï¸ Stockage des embeddings
Les embeddings sont **stockÃ©s dans MongoDB**, et non en mÃ©moire Python.

### ğŸš€ Atlas Vector Search
La recherche sÃ©mantique utilise **Atlas Vector Search** basÃ© sur l'algorithme **HNSW** (Hierarchical Navigable Small World) :
- âš¡ Plus rapide que la recherche linÃ©aire
- ğŸ“ˆ Hautement scalable
- ğŸ¯ OptimisÃ© pour les grandes dimensions vectorielles

### ğŸ” Recherche distribuÃ©e
Le moteur compare la requÃªte aux vecteurs stockÃ©s via la **similaritÃ© cosinus**, mais de maniÃ¨re **distribuÃ©e** et optimisÃ©e.

### ğŸ“Š ScalabilitÃ©
Cette approche est **scalable** : elle supporte des **dizaines de milliers Ã  des millions de documents** sans dÃ©gradation des performances.

### ğŸ¤– Pipeline RAG
Le rÃ©sultat de la recherche alimente un **modÃ¨le LLM (Llama 3)** â†’ crÃ©ation d'un pipeline **RAG (Retrieval-Augmented Generation)**.

### ğŸ“ˆ Score de pertinence
MongoDB renvoie un **score de pertinence** pour chaque document, basÃ© sur la **proximitÃ© vectorielle**.

### ğŸ’ª Robustesse
Plus robuste que la premiÃ¨re mÃ©thode, qui calculait les similaritÃ©s **manuellement en Python** et ne convenait qu'Ã  de **petits datasets**.

---

## ğŸ—ï¸ Architecture du systÃ¨me

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Query     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sentence Transformer   â”‚
â”‚  (Generate Embedding)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MongoDB Atlas          â”‚
â”‚  Vector Search (HNSW)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Top K Documents        â”‚
â”‚  (with scores)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM (Llama 3)          â”‚
â”‚  Generate Response      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

# ğŸ” Logique de la 2Ã¨me approche

---

Cette approche suit un pipeline en **4 Ã©tapes** :

---

## 1ï¸âƒ£ PrÃ©paration des donnÃ©es (offline)

* Chaque description de webtoon/manga est transformÃ©e en **embedding** grÃ¢ce au modÃ¨le `paraphrase-multilingual-mpnet-base-v2`.
* Le script `generate_embeddings.py` ajoute un champ `"embedding"` Ã  chaque document dans MongoDB.

â¡ï¸ **La base contient maintenant du texte et des vecteurs prÃªts pour la recherche sÃ©mantique.**

---

## 2ï¸âƒ£ Indexation vectorielle dans MongoDB Atlas

* Un **index vectoriel** `vector_index` est crÃ©Ã© sur le champ `"embedding"`.
* MongoDB peut dÃ©sormais effectuer une **recherche par similaritÃ©** directement dans la base.

---

## 3ï¸âƒ£ Recherche sÃ©mantique (dans Streamlit â€“ rag_manga.py)

Lorsqu'un utilisateur pose une question dans l'application :

* Le texte est converti en **vector** (`queryVec`) par le mÃªme modÃ¨le.
* MongoDB exÃ©cute un `$vectorSearch` :
   * Compare le vecteur utilisateur aux embeddings de la base.
   * Retourne les documents les plus proches + un **score de pertinence**.

â¡ï¸ **C'est MongoDB (et non Python) qui calcule la similaritÃ© cosinus via HNSW.**

---

## 4ï¸âƒ£ GÃ©nÃ©ration de rÃ©ponse (RAG)

* Les webtoons les plus pertinents sont envoyÃ©s au **LLM Llama 3** via **Groq**.
* Le modÃ¨le utilise uniquement ce contexte pour gÃ©nÃ©rer une rÃ©ponse adaptÃ©e.

â¡ï¸ **Le systÃ¨me combine retrieval + gÃ©nÃ©ration â†’ c'est du RAG.**
